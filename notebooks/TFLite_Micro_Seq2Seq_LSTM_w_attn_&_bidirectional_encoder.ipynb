{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFLite-Micro-Seq2Seq: LSTM w/ attn & bidirectional encoder.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMgYVaRumuo0"
      },
      "source": [
        "# TFLite Micro Seq2Seq: LSTM w/ attention & bidirectional encoder\n",
        "\n",
        "In this notebook, we first build an attentional LSTM-based encoder-decoder model for sequence to sequence applications such as machine translation, then we convert it to TFLite models such that we can deploy it using TFLite Micro.\n",
        "\n",
        "Full instructions including pretrained TFLite models and source code for deploying converted models to Arduino Nano 33 BLE can be found at https://github.com/da03/TFLite-Micro-Seq2Seq."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKlr9X9Tnqw_"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We fully tested our models on TensorFlow 2.1.0. We used this particular version because the latest precompiled package for Arduino is built using TF 2.1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvSkBTGZn5Ml"
      },
      "source": [
        "!apt-get -qq update && apt-get -qq install xxd\n",
        "!pip install -q tensorflow==2.1.0",
        "!pip install -U numpy==1.18.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgZHR8TO0lFF"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3lP1wMzod24"
      },
      "source": [
        "## Data\n",
        "\n",
        "Seq2seq models can be used for a variety of applications, such as machine translation, document summarization, math-image-to-LaTeX etc. For demonstration purposes, we use a number-to-word task here, where the goal is to convert a number to English words:\n",
        "\n",
        "|      Input |       Output         |\n",
        "|------------|----------------------|\n",
        "|    7929    |seven thousand nine hundred and twenty nine|\n",
        "|   842259   |eight hundred and forty two thousand two hundred and fifty nine|\n",
        "|   508217   |five hundred and eight thousand two hundred and seventeen|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2GqERhL65lW"
      },
      "source": [
        "|      Module |    Configuration |   #Parameters         |\n",
        "|------------|----------|------------|\n",
        "|    Src Embeddings    |embedding size 64| 1k |\n",
        "|   Tgt Embeddings   |embedding size 64| 2k |\n",
        "|   Encoder LSTM (l2r)   |hidden size 32| 12k |\n",
        "|   Encoder LSTM (r2l)   |hidden size 32| 12k |\n",
        "|   Decoder LSTM   |hidden size 64| 38k|\n",
        "| Total | - | 65k|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AchEgFVmL8Ae"
      },
      "source": [
        "!wget -nv -N -P data https://raw.githubusercontent.com/da03/TFLite-Micro-Seq2Seq/master/data/train.src\n",
        "!wget -nv -N -P data https://raw.githubusercontent.com/da03/TFLite-Micro-Seq2Seq/master/data/train.tgt\n",
        "!wget -nv -N -P data https://raw.githubusercontent.com/da03/TFLite-Micro-Seq2Seq/master/data/dev.src\n",
        "!wget -nv -N -P data https://raw.githubusercontent.com/da03/TFLite-Micro-Seq2Seq/master/data/dev.tgt\n",
        "!wget -nv -N -P data https://raw.githubusercontent.com/da03/TFLite-Micro-Seq2Seq/master/data/test.src\n",
        "!wget -nv -N -P data https://raw.githubusercontent.com/da03/TFLite-Micro-Seq2Seq/master/data/test.tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h-nb5PPq2V6"
      },
      "source": [
        "Let's take a look at our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW9hvVmLq1k-"
      },
      "source": [
        "with open('data/dev.src') as fsrc:\n",
        "  with open('data/dev.tgt') as ftgt:\n",
        "    print (f'{\"Source\":20s} {\"Target\":70s}')\n",
        "    for src, tgt, _ in zip(fsrc, ftgt, range(3)):\n",
        "      print (f'{src.strip():20s} {tgt.strip():70s}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8_NIsGHrXsm"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "We load and tokenize data, build vocabulary, and convert words to word ids. \n",
        "\n",
        "For target sentence, we prepend a special token `<bos>` for beginning-of-sentence, and append a special token `<eos>` for end-of-sentence. For decoder input, we remove the last token `<eos>`, and for decoder ground truth target we remove the first token `<bos>`. For example, if the sentence is `seven thousand`, then decoder input is `<bos> seven thousand`, and the decoder ground truth target is `seven thousand <eos>`.\n",
        "\n",
        "Finally, we convert preprocessed data to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_53n-wgMPdp"
      },
      "source": [
        "# Build vocabulary from training data\n",
        "src_train = []\n",
        "tgt_train = []\n",
        "\n",
        "with open('data/train.src') as fsrc:\n",
        "  with open('data/train.tgt') as ftgt:\n",
        "    for src, tgt in zip(fsrc, ftgt):\n",
        "      src_train.append(src.strip())\n",
        "      tgt_train.append('<bos> ' + tgt.strip() + ' <eos>')\n",
        "\n",
        "print (f'Size of training set: {len(src_train)}')\n",
        "src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
        "src_tokenizer.fit_on_texts(src_train)\n",
        "\n",
        "tgt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
        "tgt_tokenizer.fit_on_texts(tgt_train)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\n",
        "print(f'Size of source vocab: {SRC_VOCAB_SIZE}')\n",
        "\n",
        "TGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\n",
        "print(f'Size of target vocab: {TGT_VOCAB_SIZE}')\n",
        "\n",
        "src_vocab = []\n",
        "for word in src_tokenizer.word_index:\n",
        "  src_vocab.append(word)\n",
        "tgt_vocab = []\n",
        "for word in tgt_tokenizer.word_index:\n",
        "  tgt_vocab.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moTGLYcMRnJR"
      },
      "source": [
        "# Prepare training data\n",
        "# encoder_input_data\n",
        "tokenized_src = src_tokenizer.texts_to_sequences(src_train)\n",
        "max_len_src = max([len(x) for x in tokenized_src])\n",
        "padded_src = tf.keras.preprocessing.sequence.pad_sequences(tokenized_src, \n",
        "                                                           maxlen=max_len_src, \n",
        "                                                           padding='post')\n",
        "encoder_input_data = np.array(padded_src)\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_tgt = tgt_tokenizer.texts_to_sequences(tgt_train)\n",
        "max_len_tgt = max([len(x) for x in tokenized_tgt])\n",
        "padded_tgt = tf.keras.preprocessing.sequence.pad_sequences(tokenized_tgt, \n",
        "                                                           maxlen=max_len_tgt, \n",
        "                                                           padding='post')\n",
        "decoder_input_data = np.array(padded_tgt)\n",
        "\n",
        "# decoder_output_data\n",
        "tokenized_tgt = tgt_tokenizer.texts_to_sequences(tgt_train)\n",
        "for i in range(len(tokenized_tgt)) :\n",
        "  tokenized_tgt[i] = tokenized_tgt[i][1:]\n",
        "padded_tgt = tf.keras.preprocessing.sequence.pad_sequences(tokenized_tgt, \n",
        "                                                           maxlen=max_len_tgt, \n",
        "                                                           padding='post')\n",
        "onehot_tgt = tf.keras.utils.to_categorical(padded_tgt, TGT_VOCAB_SIZE)\n",
        "decoder_output_data = np.array(onehot_tgt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79dR5_N7smF2"
      },
      "source": [
        "# Prepare test data\n",
        "src_test = []\n",
        "tgt_test = []\n",
        "\n",
        "with open('data/test.src') as fsrc:\n",
        "  with open('data/test.tgt') as ftgt:\n",
        "    for src, tgt in zip(fsrc, ftgt):\n",
        "      src_test.append(src.strip())\n",
        "      tgt_test.append('<bos> ' + tgt.strip()+ ' <eos>')\n",
        "\n",
        "# encoder_input_data\n",
        "tokenized_src_test = src_tokenizer.texts_to_sequences(src_test)\n",
        "tokenized_src_test = [x[:max_len_src] for x in tokenized_src_test]\n",
        "padded_src_test = tf.keras.preprocessing.sequence.pad_sequences(tokenized_src_test, \n",
        "                                                                maxlen=max_len_src, \n",
        "                                                                padding='post')\n",
        "encoder_input_data_test = np.array(padded_src_test)\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_tgt_test = tgt_tokenizer.texts_to_sequences(tgt_test)\n",
        "tokenized_tgt_test = [x[:max_len_tgt] for x in tokenized_tgt_test]\n",
        "padded_tgt_test = tf.keras.preprocessing.sequence.pad_sequences(tokenized_tgt_test, \n",
        "                                                                maxlen=max_len_tgt, \n",
        "                                                                padding='post')\n",
        "decoder_input_data_test = np.array(padded_tgt_test)\n",
        "\n",
        "# decoder_output_data\n",
        "tokenized_tgt_test = tgt_tokenizer.texts_to_sequences(tgt_test)\n",
        "for i in range(len(tokenized_tgt_test)) :\n",
        "  tokenized_tgt_test[i] = tokenized_tgt_test[i][1:]\n",
        "padded_tgt_test = tf.keras.preprocessing.sequence.pad_sequences(tokenized_tgt_test, \n",
        "                                                                maxlen=max_len_tgt, \n",
        "                                                                padding='post')\n",
        "labels_test = np.array(padded_tgt_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2NFrS5duYZK"
      },
      "source": [
        "## Model\n",
        "\n",
        "We use an  LSTM-based attentional encoder-decoder model, where the encoder is a bi-directional LSTM, and the decoder is an LSTM with attention. A diagram of the model can be found below.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/da03/TFLite-Micro-Seq2Seq/main/img/encoder_decoder_attn_1layer.png\" alt=\"attentional encoder-decoder illustration\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nigh4ZYmyPxd"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For implementation, we use the builtin `tf.keras.layers.LSTMCell` for encoder LSTM cells. For decoder attentional LSTM cell, we use a customized implementation to avoid operations not supported by TFLite Micro, such as [transpose](https://github.com/tensorflow/tensorflow/issues/43472)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-pnM4ANmr4I"
      },
      "source": [
        "class AttnLSTMCell(tf.keras.layers.Layer):\n",
        "  def __init__(self, hidden_size, output_size, **kwargs):\n",
        "    self.h = hidden_size\n",
        "    self.o = output_size\n",
        "\n",
        "    self.state_size = tf.TensorShape([hidden_size])\n",
        "    self.output_size = tf.TensorShape([output_size])\n",
        "\n",
        "    super(AttnLSTMCell, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.h)\n",
        "\n",
        "  def call(self, inputs, states):\n",
        "    inputs, encoder_outputs = inputs\n",
        "    outputs, new_states = self.decoder_lstm_cell(inputs, states)\n",
        "    query = tf.keras.layers.Reshape((dec_hidden_size, 1))(outputs) # bsz, H, 1\n",
        "    values = encoder_outputs # bsz, max_len_src, H\n",
        "    keys = values\n",
        "    scores = tf.matmul(values, query) # bsz, max_len_src, 1\n",
        "    scores = tf.keras.layers.Softmax(1)(scores) # bsz, max_len_src, 1\n",
        "    context = scores * values # bsz, max_len_src, H\n",
        "    context_list = tf.split(context, num_or_size_splits=max_len_src, axis=1)\n",
        "    context = context_list[0]\n",
        "    for i in range(1, max_len_src):\n",
        "      context = context + context_list[i]\n",
        "    context = tf.keras.layers.Reshape((dec_hidden_size,))(context)\n",
        "    outputs = tf.keras.layers.Concatenate(axis=-1)([context, outputs])\n",
        "    \n",
        "    return outputs, new_states\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\"hidden_size\": self.h, \"output_size\": self.o}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFAN_48uzLC4"
      },
      "source": [
        "With the customized attentional LSTM cell, we are ready to build the full model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLoblhLcZpof"
      },
      "source": [
        "HIDDEN_SIZE = 64\n",
        "\n",
        "hidden_size = HIDDEN_SIZE\n",
        "enc_hidden_size = hidden_size // 2\n",
        "dec_hidden_size = hidden_size\n",
        "\n",
        "src_embedding_size = hidden_size\n",
        "tgt_embedding_size = hidden_size\n",
        "\n",
        "# Input tensors:\n",
        "# Encoder inputs\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(max_len_src,)) # bsz, max_len_src\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(max_len_tgt,)) # bsz, max_len_tgt\n",
        "\n",
        "# Encoder\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(SRC_VOCAB_SIZE, \n",
        "                                                    src_embedding_size) \n",
        "encoder_embeddings = encoder_embedding_layer(encoder_inputs) # bsz, max_len_src, src_embedding_size\n",
        "encoder_lstm_cell_fw = tf.keras.layers.LSTMCell(enc_hidden_size)\n",
        "encoder_lstm_cell_bw = tf.keras.layers.LSTMCell(enc_hidden_size)\n",
        "encoder_lstm_layer_fw = tf.keras.layers.RNN(encoder_lstm_cell_fw, \n",
        "                                            return_sequences=True, \n",
        "                                            return_state=True, \n",
        "                                            go_backwards=False)\n",
        "encoder_lstm_layer_bw = tf.keras.layers.RNN(encoder_lstm_cell_bw, \n",
        "                                            return_sequences=True, \n",
        "                                            return_state=True, \n",
        "                                            go_backwards=True)\n",
        "\n",
        "encoder_lstm_layer = tf.keras.layers.Bidirectional(encoder_lstm_layer_fw, \n",
        "                                                   merge_mode='concat',\n",
        "                                                   backward_layer=encoder_lstm_layer_bw)\n",
        "# Reset bidirectional since Bidirectional creates new cells\n",
        "encoder_lstm_cell_fw = encoder_lstm_layer.forward_layer.cell \n",
        "encoder_lstm_cell_bw = encoder_lstm_layer.backward_layer.cell\n",
        "encoder_outputs, encoder_states_h_fw, encoder_states_c_fw, \\\n",
        "  encoder_states_h_bw, encoder_states_c_bw \\\n",
        "                                        = encoder_lstm_layer(encoder_embeddings)\n",
        "encoder_states_h = tf.keras.layers.Concatenate(axis=1)([encoder_states_h_fw, \n",
        "                                                        encoder_states_h_bw])\n",
        "encoder_states_c = tf.keras.layers.Concatenate(axis=1)([encoder_states_c_fw, \n",
        "                                                        encoder_states_c_bw])\n",
        "encoder_states = (encoder_states_h, encoder_states_c)\n",
        "\n",
        "# Decoder\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(TGT_VOCAB_SIZE, tgt_embedding_size)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs) # bsz, max_len_tgt, tgt_embedding_size\n",
        "\n",
        "decoder_lstm_cell = AttnLSTMCell(dec_hidden_size, TGT_VOCAB_SIZE)\n",
        "decoder_proj_layer = tf.keras.layers.Dense(TGT_VOCAB_SIZE)\n",
        "\n",
        "logits = []\n",
        "decoder_state = encoder_states\n",
        "context = None\n",
        "for t in range(max_len_tgt):\n",
        "  decoder_embedding = decoder_embeddings[:, t]\n",
        "  # Feed context vector to decoder input (see model diagram)\n",
        "  if context is not None:\n",
        "    decoder_embedding = decoder_embedding + context\n",
        "  decoder_output, decoder_state = decoder_lstm_cell([decoder_embedding, encoder_outputs], decoder_state)\n",
        "  logit = decoder_proj_layer(decoder_output) # bsz, vocab_size\n",
        "  context = decoder_output[:, :dec_hidden_size]\n",
        "  logits.append(logit)\n",
        "\n",
        "decoder_logits = tf.stack(logits, 1)\n",
        "\n",
        "# Compile model\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_logits)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              experimental_run_tf_function=False)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ET07CHl1mI1"
      },
      "source": [
        "## Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4iNzcHoTnFw"
      },
      "source": [
        "# Train model\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 32\n",
        "model.fit([encoder_input_data, decoder_input_data], \n",
        "          decoder_output_data, \n",
        "          batch_size=BATCH_SIZE, \n",
        "          epochs=EPOCHS) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U7uhZaF1utc"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elwWsaVzK8Xj"
      },
      "source": [
        "# Evaluate model\n",
        "total = 0\n",
        "correct = 0\n",
        "for encoder_input_d, decoder_input_d, label_d in zip(encoder_input_data_test, \n",
        "                                                     decoder_input_data_test, \n",
        "                                                     labels_test):\n",
        "  encoder_input_d = encoder_input_d.reshape((1, -1))\n",
        "  decoder_input_d = decoder_input_d.reshape((1, -1))\n",
        "  label_d = label_d.reshape((1, -1))\n",
        "  logits = model.predict_on_batch([encoder_input_d, decoder_input_d])\n",
        "  predictions = logits.argmax(-1)\n",
        "  if (predictions == label_d).all():\n",
        "    correct += 1\n",
        "  total +=1\n",
        "print (f'Test accuracy: {100.0*correct/total}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQA2rW0G2K1K"
      },
      "source": [
        "## Conversion to TFLite\n",
        "\n",
        "With a trained model, we can deploy it to microcontrollers. First, we convert it to TFLite. In particular, we convert encoder LSTM cell and decoder LSTM cell to TFLite. We don't convert the unrolled full encoder because TFLite Micro does not support subgraphs. We don't convert Embedding layers because they are not yet supported by TFLite Micro. Later we will show a workaround for this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjMCLRbP62OE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX_b6yHIsN93"
      },
      "source": [
        "# Create tflite model\n",
        "# Encoder LSTM Cell (left-to-right)\n",
        "# Inputs\n",
        "encoder_embedding_fw = tf.keras.Input(shape=(src_embedding_size,)) # bsz, src_embedding_size\n",
        "encoder_state_h_fw = tf.keras.Input(shape=(enc_hidden_size,)) # bsz, hidden_size\n",
        "encoder_state_c_fw = tf.keras.Input(shape=(enc_hidden_size,)) # bsz, hidden_size\n",
        "\n",
        "encoder_output_fw, (encoder_state_h_out_fw, encoder_state_c_out_fw) \\\n",
        "               = encoder_lstm_cell_fw(encoder_embedding_fw, \n",
        "                                      (encoder_state_h_fw, encoder_state_c_fw))\n",
        "\n",
        "enc_fw_micro_model =  tf.keras.models.Model([encoder_embedding_fw, \n",
        "                                             encoder_state_h_fw, \n",
        "                                             encoder_state_c_fw], \n",
        "                                            [encoder_output_fw, \n",
        "                                             encoder_state_h_out_fw, \n",
        "                                             encoder_state_c_out_fw])\n",
        "enc_fw_micro_model.summary()\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(enc_fw_micro_model)\n",
        "buffer_enc_fw = converter.convert()\n",
        "open('enc_model_fw.tflite', 'wb').write(buffer_enc_fw)\n",
        "\n",
        "# Encoder LSTM Cell (right-to-left)\n",
        "# Inputs\n",
        "encoder_embedding_bw = tf.keras.Input(shape=(src_embedding_size,)) # bsz, src_embedding_size\n",
        "encoder_state_h_bw = tf.keras.Input(shape=(enc_hidden_size,)) # bsz, hidden_size\n",
        "encoder_state_c_bw = tf.keras.Input(shape=(enc_hidden_size,)) # bsz, hidden_size\n",
        "\n",
        "encoder_output_bw, (encoder_state_h_out_bw, encoder_state_c_out_bw) \\\n",
        "               = encoder_lstm_cell_bw(encoder_embedding_bw, \n",
        "                                      (encoder_state_h_bw, encoder_state_c_bw))\n",
        "\n",
        "enc_bw_micro_model =  tf.keras.models.Model([encoder_embedding_bw, \n",
        "                                             encoder_state_h_bw, \n",
        "                                             encoder_state_c_bw], \n",
        "                                            [encoder_output_bw, \n",
        "                                             encoder_state_h_out_bw, \n",
        "                                             encoder_state_c_out_bw])\n",
        "enc_bw_micro_model.summary()\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(enc_bw_micro_model)\n",
        "buffer_enc_bw = converter.convert()\n",
        "open('enc_model_bw.tflite', 'wb').write(buffer_enc_bw)\n",
        "\n",
        "# Decoder LSTM Cell\n",
        "# Inputs\n",
        "encoder_output = tf.keras.Input(shape=(max_len_src, dec_hidden_size,)) # bsz, tgt_embedding_size\n",
        "decoder_embedding = tf.keras.Input(shape=(tgt_embedding_size,)) # bsz, tgt_embedding_size\n",
        "decoder_state_h = tf.keras.Input(shape=(dec_hidden_size,)) # bsz, hidden_size\n",
        "decoder_state_c = tf.keras.Input(shape=(dec_hidden_size,)) # bsz, hidden_size\n",
        "\n",
        "decoder_output, (decoder_state_h_out, decoder_state_c_out) \\\n",
        "              = decoder_lstm_cell([decoder_embedding, encoder_output], \n",
        "                                  (decoder_state_h, decoder_state_c))\n",
        "decoder_context = decoder_output[:, :dec_hidden_size]\n",
        "decoder_logit = decoder_proj_layer(decoder_output) # bsz, tgt_vocab_size\n",
        "\n",
        "\n",
        "dec_micro_model =  tf.keras.models.Model([encoder_output, \n",
        "                                          decoder_embedding, \n",
        "                                          decoder_state_h, \n",
        "                                          decoder_state_c], \n",
        "                                         [decoder_logit, \n",
        "                                          decoder_context, \n",
        "                                          decoder_state_h_out, \n",
        "                                          decoder_state_c_out])\n",
        "dec_micro_model.summary()\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(dec_micro_model)\n",
        "buffer_dec = converter.convert()\n",
        "open('dec_model.tflite' , 'wb').write(buffer_dec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbEsNofq4GWL"
      },
      "source": [
        "## Conversion to TFLite Micro (Optional)\n",
        "\n",
        "Now we can further convert our models to TFLite Micro and deploy to Arduino Nano 33 BLE. One issue is that TFLite Micro does not support Embeddings (because it does not support gather operations). To circumvent this issue, we directly implement embeddings in C++. The generated C++ files can be found in folder `c_src`.\n",
        "\n",
        "Alternatively, you can also skip this section and jump to the next section for doing inference in TFLite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrkpnkt2HSWs"
      },
      "source": [
        "# Create C files for Arduino\n",
        "c_folder = \"c_src\"\n",
        "!mkdir -p {c_folder}\n",
        "model_h_file = os.path.join(c_folder, 'model.h')\n",
        "model_cpp_file = os.path.join(c_folder, 'model.cpp')\n",
        "# model.h\n",
        "for word, index in tgt_tokenizer.word_index.items():\n",
        "  if word == '<bos>':\n",
        "    bos_word_id = index\n",
        "  elif word == '<eos>':\n",
        "    eos_word_id = index\n",
        "model_h_str = f\"\"\"\n",
        "#ifndef MODEL_H_\n",
        "#define MODEL_H_\n",
        "\n",
        "#include \"Arduino.h\"\n",
        "#include \"tensorflow/lite/c/common.h\"\n",
        "\n",
        "extern const unsigned char g_enc_model_fw[];\n",
        "extern const unsigned char g_enc_model_bw[];\n",
        "extern const unsigned char g_dec_model[];\n",
        "void set_enc_embed(TfLiteTensor* ptr, String token);\n",
        "void set_dec_embed(TfLiteTensor* ptr, String token);\n",
        "String id_to_word(int idx);\n",
        "\n",
        "const int max_len_src = {max_len_src};\n",
        "const int max_len_tgt = {max_len_tgt};\n",
        "const int bos_word_id = {bos_word_id};\n",
        "const int eos_word_id = {eos_word_id};\n",
        "const int enc_hidden_size = {enc_hidden_size};\n",
        "const int dec_hidden_size = {dec_hidden_size};\n",
        "const int src_embedding_size = {src_embedding_size};\n",
        "const int tgt_embedding_size = {tgt_embedding_size};\n",
        "const int src_vocab_size = {SRC_VOCAB_SIZE};\n",
        "const int tgt_vocab_size = {TGT_VOCAB_SIZE};\n",
        "#endif\n",
        "\"\"\"\n",
        "\n",
        "with open(model_h_file, 'w') as fout:\n",
        "  fout.write(model_h_str)\n",
        "\n",
        "# model.cpp - enc rnn left-to-right\n",
        "!xxd -i  enc_model_fw.tflite > tmp\n",
        "\n",
        "with open('tmp') as fin:\n",
        "  with open(model_cpp_file, 'w') as fout:\n",
        "    text = fin.read().strip()\n",
        "    lines = list(text.split('\\n'))\n",
        "    fout.write('#include \"model.h\"\\n')\n",
        "    fout.write('alignas(8) const unsigned char g_enc_model_fw[] = {\\n')\n",
        "    for line in lines[1:-1]:\n",
        "      fout.write(line + '\\n')\n",
        "\n",
        "# model.cpp - enc rnn right-to-left\n",
        "!xxd -i  enc_model_bw.tflite > tmp\n",
        "\n",
        "with open('tmp') as fin:\n",
        "  with open(model_cpp_file, 'a') as fout:\n",
        "    text = fin.read().strip()\n",
        "    lines = list(text.split('\\n'))\n",
        "    fout.write('alignas(8) const unsigned char g_enc_model_bw[] = {\\n')\n",
        "    for line in lines[1:-1]:\n",
        "      fout.write(line + '\\n')\n",
        "\n",
        "# model.cpp - dec rnn\n",
        "!xxd -i  dec_model.tflite > tmp\n",
        "\n",
        "with open('tmp') as fin:\n",
        "  with open(model_cpp_file, 'a') as fout:\n",
        "    text = fin.read().strip()\n",
        "    lines = list(text.split('\\n'))\n",
        "    fout.write('alignas(8) const unsigned char g_dec_model[] = {\\n')\n",
        "    for line in lines[1:-1]:\n",
        "      fout.write(line + '\\n')\n",
        "\n",
        "# model.cpp - embeddings\n",
        "def to_c(array):\n",
        "  return '{' + ','.join([ '{' + ','.join([str(i) for i in item]) + '}' for item in array]) + '}'\n",
        "\n",
        "src_embeddings_str = to_c(model.trainable_variables[0].numpy())\n",
        "src_vocab_str = '{' + \",\".join(['\"' + item + '\"' for item in src_vocab]) + '}'\n",
        "enc_embedding_str = f\"\"\"\n",
        "void set_enc_embed(TfLiteTensor* ptr, String token) {{\n",
        "  float embeddings[src_vocab_size][src_embedding_size] = {src_embeddings_str};\n",
        "  const char *words[src_vocab_size-1] = {src_vocab_str};\n",
        "  int word_id = 0;\n",
        "  for (int i = 1; i < src_vocab_size; i++) {{\n",
        "    String token2 = words[i-1];\n",
        "    if (token == token2) {{\n",
        "      word_id = i;\n",
        "      break;\n",
        "    }}\n",
        "  }}\n",
        "  for (int i = 0; i < src_embedding_size; i++) {{\n",
        "    ptr->data.f[i] = embeddings[word_id][i];\n",
        "  }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "with open(model_cpp_file, 'a') as fout:\n",
        "  fout.write('\\n')\n",
        "  fout.write(enc_embedding_str)\n",
        "\n",
        "tgt_embeddings_str = to_c(model.trainable_variables[1].numpy())\n",
        "tgt_vocab_str = '{' + \",\".join(['\"' + item + '\"' for item in tgt_vocab]) + '}'\n",
        "dec_embedding_str = f\"\"\"\n",
        "void set_dec_embed(TfLiteTensor* ptr, String token) {{\n",
        "  float embeddings[tgt_vocab_size][tgt_embedding_size] = {tgt_embeddings_str};\n",
        "  const char *words[tgt_vocab_size-1] = {tgt_vocab_str};\n",
        "  int word_id = 0;\n",
        "  for (int i = 1; i < tgt_vocab_size; i++) {{\n",
        "    String token2 = words[i-1];\n",
        "    if (token == token2) {{\n",
        "      word_id = i;\n",
        "      break;\n",
        "    }}\n",
        "  }}\n",
        "  for (int i = 0; i < tgt_embedding_size; i++) {{\n",
        "    ptr->data.f[i] = embeddings[word_id][i];\n",
        "  }}\n",
        "}}\n",
        "\n",
        "String id_to_word(int idx) {{\n",
        "  const char *words[tgt_vocab_size-1] = {tgt_vocab_str};\n",
        "  if (idx == 0) {{\n",
        "    return String(\"pad\");\n",
        "  }}\n",
        "  String str = words[idx-1];\n",
        "  return str;\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "with open(model_cpp_file, 'a') as fout:\n",
        "  fout.write('\\n')\n",
        "  fout.write(dec_embedding_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YSkmg2S509V"
      },
      "source": [
        "Now, we can download the generated C++ files and save them to the `src` folder of the Arduino project and upload it to Arduino Nano 33 BLE. Please refer to https://github.com/da03/TFLite-Micro-Seq2Seq for details on how to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9g_8sR7WWf3"
      },
      "source": [
        "## Inference using TFLite (Optional)\n",
        "\n",
        "This section shows how to perform inference using TFLite. The workflow is essentially the same as what we used in the Arduino project, so it can also be used for debugging purposes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P_wDD554q9O"
      },
      "source": [
        "def sentence_to_ids(sentence):\n",
        "  words = sentence.lower().split()\n",
        "  tokens_list = []\n",
        "  for word in words:\n",
        "    tokens_list.append( src_tokenizer.word_index[word]) \n",
        "  return tf.keras.preprocessing.sequence.pad_sequences([tokens_list], \n",
        "                                                       maxlen=max_len_src,\n",
        "                                                       padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPUkf8_lPwEJ"
      },
      "source": [
        "# Initialize the TFLite interpreters\n",
        "interpreter_enc_fw = tf.lite.Interpreter(model_content=buffer_enc_fw)\n",
        "interpreter_enc_fw.allocate_tensors()\n",
        "\n",
        "interpreter_enc_bw = tf.lite.Interpreter(model_content=buffer_enc_bw)\n",
        "interpreter_enc_bw.allocate_tensors()\n",
        "\n",
        "interpreter_dec = tf.lite.Interpreter(model_content=buffer_dec)\n",
        "interpreter_dec.allocate_tensors()\n",
        "\n",
        "input_details_enc_fw = interpreter_enc_fw.get_input_details()\n",
        "output_details_enc_fw = interpreter_enc_fw.get_output_details()\n",
        "\n",
        "input_details_enc_bw = interpreter_enc_bw.get_input_details()\n",
        "output_details_enc_bw = interpreter_enc_bw.get_output_details()\n",
        "\n",
        "input_details_dec = interpreter_dec.get_input_details()\n",
        "output_details_dec = interpreter_dec.get_output_details()\n",
        "\n",
        "# Get source sentence\n",
        "encoder_input = sentence_to_ids(input('Enter tokenized source sentence: '))\n",
        "\n",
        "h_fw = np.zeros((1, enc_hidden_size))\n",
        "c_fw = np.zeros((1, enc_hidden_size))\n",
        "memory_bank = np.zeros((1, max_len_src, dec_hidden_size))\n",
        "for t in range(max_len_src):\n",
        "  embeddings = model.trainable_variables[0].numpy()[encoder_input[0][t]].reshape((1, -1))\n",
        "  interpreter_enc_fw.set_tensor(input_details_enc_fw[0][\"index\"], embeddings.astype(np.float32))\n",
        "  interpreter_enc_fw.set_tensor(input_details_enc_fw[1][\"index\"], h_fw.astype(np.float32))\n",
        "  interpreter_enc_fw.set_tensor(input_details_enc_fw[2][\"index\"], c_fw.astype(np.float32))\n",
        "  interpreter_enc_fw.invoke()\n",
        "  out_fw = interpreter_enc_fw.get_tensor(output_details_enc_fw[0][\"index\"])\n",
        "  h_fw = interpreter_enc_fw.get_tensor(output_details_enc_fw[1][\"index\"])\n",
        "  c_fw = interpreter_enc_fw.get_tensor(output_details_enc_fw[2][\"index\"])\n",
        "  memory_bank[:, t, :enc_hidden_size] = out_fw\n",
        "\n",
        "h_bw = np.zeros((1, enc_hidden_size))\n",
        "c_bw = np.zeros((1, enc_hidden_size))\n",
        "for t in range(max_len_src-1, -1, -1):\n",
        "  embeddings = model.trainable_variables[0].numpy()[encoder_input[0][t]].reshape((1, -1))\n",
        "  interpreter_enc_bw.set_tensor(input_details_enc_bw[0][\"index\"], embeddings.astype(np.float32))\n",
        "  interpreter_enc_bw.set_tensor(input_details_enc_bw[1][\"index\"], h_bw.astype(np.float32))\n",
        "  interpreter_enc_bw.set_tensor(input_details_enc_bw[2][\"index\"], c_bw.astype(np.float32))\n",
        "  interpreter_enc_bw.invoke()\n",
        "  out_bw = interpreter_enc_bw.get_tensor(output_details_enc_bw[0][\"index\"])\n",
        "  h_bw = interpreter_enc_bw.get_tensor(output_details_enc_bw[1][\"index\"])\n",
        "  c_bw = interpreter_enc_bw.get_tensor(output_details_enc_bw[2][\"index\"])\n",
        "  memory_bank[:, t, enc_hidden_size:] = out_bw\n",
        "\n",
        "decoded_translation = ''\n",
        "prev_word_id = bos_word_id\n",
        "h = np.concatenate([h_fw, h_bw], axis=-1)\n",
        "c = np.concatenate([c_fw, c_bw], axis=-1)\n",
        "context = None\n",
        "while True:\n",
        "  embeddings = model.trainable_variables[1].numpy()[prev_word_id].reshape((1, -1))\n",
        "  if context is not None:\n",
        "    embeddings = embeddings + context.reshape((1, -1))\n",
        "  interpreter_dec.set_tensor(input_details_dec[0][\"index\"], memory_bank.astype(np.float32))\n",
        "  interpreter_dec.set_tensor(input_details_dec[1][\"index\"], embeddings.astype(np.float32))\n",
        "  interpreter_dec.set_tensor(input_details_dec[2][\"index\"], h.astype(np.float32))\n",
        "  interpreter_dec.set_tensor(input_details_dec[3][\"index\"], c.astype(np.float32))\n",
        "  interpreter_dec.invoke()\n",
        "  dec_outputs = interpreter_dec.get_tensor(output_details_dec[2][\"index\"])\n",
        "  context = interpreter_dec.get_tensor(output_details_dec[3][\"index\"])\n",
        "  h = interpreter_dec.get_tensor(output_details_dec[0][\"index\"])\n",
        "  c = interpreter_dec.get_tensor(output_details_dec[1][\"index\"])\n",
        "  sampled_word_id = np.argmax( dec_outputs[0])\n",
        "  prev_word_id = sampled_word_id\n",
        "  if sampled_word_id == eos_word_id or len(decoded_translation.split()) > max_len_tgt:\n",
        "    break\n",
        "  for word, index in tgt_tokenizer.word_index.items() :\n",
        "    if sampled_word_id == index:\n",
        "        decoded_translation += ' ' + word\n",
        "        break\n",
        "\n",
        "print(decoded_translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9k4nvJHkVQP"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "* This notebook is based on [Chatbot using seq2seq LSTM models](https://colab.research.google.com/drive/1FKhOYhOz8d6BKLVVwL1YMlmoFQ2ML1DS).\n",
        "* The number-to-words task used here comes from CS187 at Harvard.\n",
        "* This project is a course project of [CS249](https://scholar.harvard.edu/vijay-janapa-reddi/classes/cs249r-tinyml) at Harvard."
      ]
    }
  ]
}
